
# TODO Include the rest of parameters...

# Argumento -T, nombre del fichero de test. Por defecto, utilizar los datos 
# de entrenamiento

@click.option('--test_file', '-T', default=None, required=False, show_default=True,
    help=u'Name of the file with test data.')    

# Argumento -c, indica si el problema es de clasificacion.
# tipo booleano. Por defecto, problema de regresion

@click.option('--classification', '-c', default=False, required=False, show_default=True,
    is_flag=True, help=u'Indicates if the problem is classification o regresion.')

# Argumento -r, Razon en tanto por uno de neuronas RBF respecto al total de patrones.
# Por defecto, 0.1 capa oculta

@click.option('--ratio_rbf', '-r', default=0.1, required=False, show_default=True,
    help=u'Ratio of RBF neurons with respect to the total number of patterns.')

# Argumento -l, boolenao que indica si el tipo de regularizacion. Por defecto, L1

@click.option('--l2', '-l', default=False, required=False, show_default=True,
    is_flag=True, help=u'Indicates if the regularization is L2 or L1.')

# Argumento -e, valor de eta. Por defecto, 0.01

@click.option('--eta', '-e', default=0.01, required=False, show_default=True,
    help=u'Value of eta.')

# Argumento -f, activa el cálculo de métricas de rendimiento por grupos.
# Asume que el grupo esta almacenado como ultima variable de las variables de entrada
# Por defecto, esta desactivado

@click.option('--fairness', '-f', default=False, required=False, show_default=True,
    is_flag=True, help=u'Indicates if the performance metrics by groups are calculated.')

# Argumento -o, numero de columnas de salida del conjunto de datos y siempre al final.
# Por defecto, 1

@click.option('--outputs', '-o', default=1, required=False, show_default=True,
    help=u'Number of output columns of the dataset and always at the end.')


######################################################################################

def train_rbf(train_file, test_file, classification, ratio_rbf, l2, eta, fairness, outputs, model_file=""):
    #TODO: Obtain num_rbf from ratio_rbf
    num_rbf = int(ratio_rbf * len(train_inputs)) 

    TODO: Obtain the distances from the centroids to the test patterns
        and obtain the R matrix for the test set
    test_distances = kmeans.transform(test_inputs)
    test_r_matrix = calculate_r_matrix(test_distances, radii)

    # TODO Group label (we assume it is in the last column of input data): 
            # 1 women / 0 men
    group_label = test_inputs[:,-1]

    # not clasificacion
    TODO: Obtain the predictions for training and test and calculate
              the MSE

    train_predictions = np.matmul(r_matrix, coefficients) # Predicciones de los patrones de entrenamiento
    test_predictions = np.matmul(test_r_matrix, coefficients) # Predicciones de los patrones de test
    #print(np.array_str(test_predictions, suppress_small=True))
    train_mse = mean_squared_error(train_predictions, train_outputs) # MSE de los patrones de entrenamiento
    test_mse = mean_squared_error(test_predictions, test_outputs) # MSE de los patrones de test
    train_ccr = 0 # CCR de los patrones de entrenamiento
    test_ccr = 0 # CCR de los patrones de test

    # clasificacion
    TODO: Obtain the predictions for training and test and calculate
              the CCR. Obtain also the MSE, but comparing the obtained
              probabilities and the target probabilities


        log_b = OneHotEncoder() # Codificador de etiquetas de clase
        train_b_outputs = log_b.fit_transform(train_outputs).toarray() # Codificación de las etiquetas de clase de entrenamiento
        test_b_outputs = log_b.fit_transform(test_outputs).toarray() # Codificación de las etiquetas de clase de test
        train_ccr = logreg.score(r_matrix, train_outputs) * 100 # CCR de los patrones de entrenamiento
        test_ccr = logreg.score(test_r_matrix, test_outputs) * 100 # CCR de los patrones de test
        train_mse = mean_squared_error(train_b_outputs, logreg.predict_proba(r_matrix)) # MSE de los patrones de entrenamiento
        test_mse = mean_squared_error(test_b_outputs, logreg.predict_proba(test_r_matrix)) # MSE de los patrones de test
        predict = logreg.predict(test_r_matrix) # Predicciones de los patrones de test
        cm = confusion_matrix(test_outputs, predict) # Matriz de confusión
        print('Matriz de confusion')
        print(cm)

############################################################################################

def read_data(train_file, test_file, outputs):
    """ Read the input data
        It receives the name of the input data file names (training and test)
        and returns the corresponding matrices

        Parameters
        ----------
        train_file: string
            Name of the training file
        test_file: string
            Name of the test file
        outputs: int
            Number of variables to be used as outputs
            (all at the end of the matrix).
              
        Returns
        -------
        train_inputs: array, shape (n_train_patterns,n_inputs)
            Matrix containing the inputs for the training patterns
        train_outputs: array, shape (n_train_patterns,n_outputs)
            Matrix containing the outputs for the training patterns
        test_inputs: array, shape (n_test_patterns,n_inputs)
            Matrix containing the inputs for the test patterns
        test_outputs: array, shape (n_test_patterns,n_outputs)
            Matrix containing the outputs for the test patterns
    """

    #TODO: Complete the code of the function
    df_train = pd.read_csv(train_file, header=None) # Lectura del fichero de entrenamiento
    df_test = pd.read_csv(test_file, header=None) # Lectura del fichero de test
    train = df_train.to_numpy().astype(np.float) # Conversión a matriz de numpy del fichero de entrenamiento a float
    test = df_test.to_numpy().astype(np.float) # Conversión a matriz de numpy del fichero de test a float
    train_inputs = train[:, 0:-outputs] # Matriz de patrones de entrenamiento
    train_outputs = train[:, train_inputs.shape[1]:] # Matriz de salidas de entrenamiento
    test_inputs = test[:, 0:-outputs] # Matriz de patrones de test
    test_outputs = test[:, test_inputs.shape[1]:] # Matriz de salidas de test

    return train_inputs, train_outputs, test_inputs, test_outputs
    # return train_results, test_results 

#####################################################################
def init_centroids_classification(train_inputs, train_outputs, num_rbf):
    """ Initialize the centroids for the case of classification
        This method selects in a stratified num_rbf patterns.

        Parameters
        ----------
        train_inputs: array, shape (n_patterns,n_inputs)
            Matrix with all the input variables
        train_outputs: array, shape (n_patterns,n_outputs)
            Matrix with the outputs of the dataset
        num_rbf: int
            Number of RBFs to be used in the network
            
        Returns
        -------
        centroids: array, shape (num_rbf,n_inputs)
            Matrix with all the centroids already selected
    """
    
    #TODO: Complete the code of the function
    centroids, x_test, y_train, y_test = train_test_split(train_inputs, train_outputs, train_size=num_rbf, stratify=train_outputs) # Selección de los centroides
    return centroids

###############################################################################


def clustering(classification, train_inputs, train_outputs, num_rbf):
    """ It applies the clustering process
        A clustering process is applied to set the centers of the RBFs.
        In the case of classification, the initial centroids are set
        using the method init_centroids_classification(). 
        In the case of regression, the centroids have to be set randomly.

        Parameters
        ----------
        classification: bool
            True if it is a classification problem
        train_inputs: array, shape (n_patterns,n_inputs)
            Matrix with all the input variables
        train_outputs: array, shape (n_patterns,n_outputs)
            Matrix with the outputs of the dataset
        num_rbf: int
            Number of RBFs to be used in the network
            
        Returns
        -------
        kmeans: sklearn.cluster.KMeans
            KMeans object after the clustering
        distances: array, shape (n_patterns,num_rbf)
            Matrix with the distance from each pattern to each RBF center
        centers: array, shape (num_rbf,n_inputs)
            Centers after the clustering
    """

    #TODO: Complete the code of the function
    if classification == True:
        centroids = init_centroids_classification(train_inputs, train_outputs, num_rbf)
        kmeans = KMeans(n_clusters=num_rbf, init=centroids, n_init=1, max_iter=500) # Clustering
    else:
        kmeans = KMeans(n_clusters=num_rbf, init='random', n_init=1, max_iter=500) # Clustering

    kmeans.fit(train_inputs) # Ajuste del clustering
    distances = kmeans.transform(train_inputs) # Distancias de los patrones a los centroides
    centers = kmeans.cluster_centers_ # Centroides
    return kmeans, distances, centers

######################################################################

def calculate_radii(centers, num_rbf):
    """ It obtains the value of the radii after clustering
        This methods is used to heuristically obtain the radii of the RBFs
        based on the centers

        Parameters
        ----------
        centers: array, shape (num_rbf,n_inputs)
            Centers from which obtain the radii
        num_rbf: int
            Number of RBFs to be used in the network
            
        Returns
        -------
        radii: array, shape (num_rbf,)
            Array with the radius of each RBF
    """

    #TODO: Complete the code of the function
    dist = squareform(pdist(centers)) # Distancias entre los centroides
    radii = np.array([], dtype=np.float64) # Array de radios de los RBFs

    for i in range(0,num_rbf):
        media = sum(dist[i])/(2*(num_rbf-1)) # Media de las distancias de cada centroide a los demás
        radii = np.append(radii, media) # Añadir el radio al array de radios
    return radii

##############################################################################

def calculate_r_matrix(distances, radii):
    """ It obtains the R matrix
        This method obtains the R matrix (as explained in the slides),
        which contains the activation of each RBF for each pattern, including
        a final column with ones, to simulate bias
        
        Parameters
        ----------
        distances: array, shape (n_patterns,num_rbf)
            Matrix with the distance from each pattern to each RBF center
        radii: array, shape (num_rbf,)
            Array with the radius of each RBF
            
        Returns
        -------
        r_matrix: array, shape (n_patterns,num_rbf+1)
            Matrix with the activation of every RBF for every pattern. Moreover
            we include a last column with ones, which is going to act as bias
    """

    #TODO: Complete the code of the function
    r_matrix = np.power(distances,2) # Elevar al cuadrado las distancias de los patrones a los centroides
    
    for i in range(r_matrix.shape[1]): # Dividir entre el cuadrado del radio de cada RBF
        r_matrix[:,i] = np.divide(r_matrix[:,i], np.power(radii[i],2))
    
    r_matrix = np.exp(-r_matrix) # Aplicar la función exponencial
    ones = np.ones((r_matrix.shape[0],1)) # Array de unos para la columna de bias
    r_matrix = np.hstack((r_matrix, ones)) # Añadir la columna de unos a la matriz R
    return r_matrix

#######################################################################################

def invert_matrix_regression(r_matrix, train_outputs):
    """ Inversion of the matrix for regression case
        This method obtains the pseudoinverse of the r matrix and multiplies
        it by the targets to obtain the coefficients in the case of linear
        regression
        
        Parameters
        ----------
        r_matrix: array, shape (n_patterns,num_rbf+1)
            Matrix with the activation of every RBF for every pattern. Moreover
            we include a last column with ones, which is going to act as bias
        train_outputs: array, shape (n_patterns,n_outputs)
            Matrix with the outputs of the dataset
              
        Returns
        -------
        coefficients: array, shape (n_outputs,num_rbf+1)
            For every output, values of the coefficients for each RBF and value 
            of the bias 
    """

    #TODO: Complete the code of the function
    pseudo_inverse = np.linalg.pinv(r_matrix) # Pseudoinversa de la matriz R
    coefficients = np.matmul(pseudo_inverse, train_outputs) # Multiplicar la pseudoinversa por las salidas
    return coefficients


def logreg_classification(matriz_r, train_outputs, l2, eta):
    """ Performs logistic regression training for the classification case
        It trains a logistic regression object to perform classification based
        on the R matrix (activations of the RBFs together with the bias)
        
        Parameters
        ----------
        r_matrix: array, shape (n_patterns,num_rbf+1)
            Matrix with the activation of every RBF for every pattern. Moreover
            we include a last column with ones, which is going to act as bias
        train_outputs: array, shape (n_patterns,n_outputs)
            Matrix with the outputs of the dataset
        l2: bool
            True if we want to use L2 regularization for logistic regression 
            False if we want to use L1 regularization for logistic regression
        eta: float
            Value of the regularization factor for logistic regression
              
        Returns
        -------
        logreg: sklearn.linear_model.LogisticRegression
            Scikit-learn logistic regression model already trained
    """

    #TODO: Complete the code of the function
    if l2 == True: # Si se quiere usar L2
        logreg = LogisticRegression(C=(1/eta), solver='liblinear') # Crear el objeto de regresión logística con L2 y el valor de eta dado por parámetro 
    else: # Si se quiere usar L1
        logreg = LogisticRegression(penalty='l1', C=(1/eta), solver='liblinear') # Crear el objeto de regresión logística con L1 y el valor de eta dado por parámetro
    logreg.fit(matriz_r, train_outputs.ravel()) # Ajustar el modelo de regresión logística con la matriz R y las salidas del dataset de entrenamiento 

    return logreg

